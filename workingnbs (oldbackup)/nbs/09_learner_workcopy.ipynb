{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b172ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e8f8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import math,torch,matplotlib.pyplot as plt\n",
    "import fastcore.all as fc\n",
    "from collections.abc import Mapping\n",
    "from operator import attrgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from miniai.conv import *\n",
    "\n",
    "from fastprogress import progress_bar,master_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2cfc67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import torchvision.transforms.functional as TF\n",
    "from contextlib import contextmanager\n",
    "from torch import nn,tensor\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "import logging\n",
    "from fastcore.test import test_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c1d7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84a947f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228d96a-64e6-4b16-8d96-67c9fa505366",
   "metadata": {},
   "source": [
    "What are we doing here? A learner is something that we build, that will allow us to try anything. And on top of that we'll build things that will allow us to introspect on what goes on inside our model, and that will allow us to do multiprocess CUDA, add data augmentation, try a variety of architectures quickly, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5eea66",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27649f6-533d-4b2c-9c37-53e89d721205",
   "metadata": {},
   "source": [
    "We'll be building this from scratch - and we start with Fashion MNIST as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f13b30cd-7403-4b82-a40d-cd12ada3994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b22868a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ad7ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[x] = [torch.flatten(TF.to_tensor(o)) for o in b[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad4d2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=1024\n",
    "tds = dsd.with_transform(transformi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc2daf3e-78f1-43f4-9476-54bb9e265dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DataLoaders:\n",
    "    def __init__(self, *dls): self.train, self.valid = dls[:2]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dd(cls, dd, batch_size, as_tuple=True):\n",
    "        return cls(*[DataLoader(ds, batch_size, collate_fn=collate_dict(ds)) for ds in dd.values()])\n",
    "#why was this not in the final notebook??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f396194-3eb0-4d8e-8e55-11329d6fa2e8",
   "metadata": {},
   "source": [
    "`@classmethod` is what allows us to say `DataLoaders.something`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe348ff6-ec9d-4c0f-9e8c-b869db1f2c88",
   "metadata": {},
   "source": [
    "Running the `class DataLoaders` here in a custom way is leading to an error - that in `from_dd()`, `Dataloader` is not well defined. But without it, it is running perfectly. I guess the correct function was exported using nbdev and is being imported at the top. Anyway. Moving on!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cb11029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 784]), tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = DataLoaders.from_dd(tds, bs)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape, yb[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9816f720-a8c1-4522-a5fd-a988c9d0c2e2",
   "metadata": {},
   "source": [
    "Below is a very simple learner. This will essentially replace the `fit()` function. A learner is essentially something that will train or learn a particular model, using a particular set of Dataloaders, a particular loss function, some particular learning rate, and optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4d3fc-9b84-4d3c-810a-a21ddf2b0595",
   "metadata": {},
   "source": [
    "The basic idea with a class is to think about the info it's gonna need, and then put that in the constructor, store it away using `fc.store_attr()`. \n",
    "Then we have the `fit()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d733c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD): fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.xb,self.yb = to_device(self.batch)\n",
    "        self.preds = self.model(self.xb) #call model\n",
    "        self.loss = self.loss_func(self.preds, self.yb) #call loss function\n",
    "        if self.model.training: #if trsining\n",
    "            self.loss.backward() #do backward step\n",
    "            self.opt.step()      #do opt step\n",
    "            self.opt.zero_grad()\n",
    "        with torch.no_grad(): self.calc_stats()\n",
    "\n",
    "    def calc_stats(self):\n",
    "        acc = (self.preds.argmax(dim=1)==self.yb).float().sum()\n",
    "        self.accs.append(acc)\n",
    "        n = len(self.xb)\n",
    "        self.losses.append(self.loss*n)\n",
    "        self.ns.append(n)\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.training = train\n",
    "        dl = self.dls.train if train else self.dls.valid\n",
    "        for self.num,self.batch in enumerate(dl): self.one_batch()\n",
    "        n = sum(self.ns)\n",
    "        print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)\n",
    "    \n",
    "    def fit(self, n_epochs):\n",
    "        self.accs,self.losses,self.ns = [],[],[] #accuracy tracking\n",
    "        self.model.to(def_device) #put the model on our device\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr) #create optimizer\n",
    "        self.n_epochs = n_epochs #store how many epochs we are doing\n",
    "        for self.epoch in range(n_epochs):\n",
    "            self.one_epoch(True)  #for each epoch, call the one_epoch function\n",
    "            with torch.no_grad(): self.one_epoch(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab44c54-eb67-4b28-9248-cb2b952b753c",
   "metadata": {},
   "source": [
    "Now, define the MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8edf58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,nh = 28*28,50\n",
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be2af2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True 1.1816412760416666 0.6124333333333334\n",
      "0 False 1.1253145089285714 0.6247857142857143\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f49568-7371-4cfd-ad94-b8fda78ab248",
   "metadata": {},
   "source": [
    "Now, try using more than one worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "952339b7-5b5d-4717-a756-7f304cb3cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DataLoaders:\n",
    "    def __init__(self, *dls): self.train, self.valid = dls[:2]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dd(cls, dd, batch_size, as_tuple=True):\n",
    "        return cls(*[DataLoader(ds, batch_size, num_workers=4, collate_fn=collate_dict(ds)) for ds in dd.values()])\n",
    "#why was this not in the final notebook??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1b8b302-7909-4d30-bbbf-ebc5a17e386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 784]), tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = DataLoaders.from_dd(tds, bs)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ea6ea32-2d42-44fa-9bcb-75654471be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,nh = 28*28,50\n",
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "939c8c12-1b36-4b98-98bf-ce904cf7a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True 0.7168861979166666 0.7397166666666667\n",
      "0 False 0.70840703125 0.742\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4907b-f06e-401b-a269-092747168237",
   "metadata": {},
   "source": [
    "That was a very basic learner, one that can fit the model to data! A good representation of what a basic learner should look like, but it's not very flexible. It can't be used with our autoencoder, for example - because we can't change what we're predicting with, or calculating with. But it's a start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82718c6d",
   "metadata": {},
   "source": [
    "## Basic Callbacks Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19443abb-c2dc-4b4f-b63a-3d2bb552c862",
   "metadata": {},
   "source": [
    "This is key - experiment with this notebook, and try and create really simple versions of these functions. For example, `identity()`. Ref [Lesson 15](https://youtu.be/0Hi2r4CaHvk?si=pYodSpomGCOL_jva&t=5631).  \n",
    "(I don't see the `identity()` here - okay that's because of the nb being outdated. Chill just do my best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "534c00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "class CancelEpochException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee43512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Callback(): order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7439ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None: method(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "583ce114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionCB(Callback):\n",
    "    def before_fit(self, learn): self.count = 0\n",
    "    def after_batch(self, learn): self.count += 1\n",
    "    def after_fit(self, learn): print(f'Completed {self.count} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2b41ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1 batches\n"
     ]
    }
   ],
   "source": [
    "cbs = [CompletionCB()]\n",
    "run_cbs(cbs, 'before_fit')\n",
    "run_cbs(cbs, 'after_batch')\n",
    "run_cbs(cbs, 'after_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.preds = self.model(self.batch[0])\n",
    "        self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        try:\n",
    "            self.callback('before_epoch')\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                try:\n",
    "                    self.callback('before_batch')\n",
    "                    self.one_batch()\n",
    "                    self.callback('after_batch')\n",
    "                except CancelBatchException: pass\n",
    "            self.callback('after_epoch')\n",
    "        except CancelEpochException: pass\n",
    "    \n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        try:\n",
    "            self.callback('before_fit')\n",
    "            for self.epoch in self.epochs:\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "            self.callback('after_fit')\n",
    "        except CancelFitException: pass\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38009cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "m,nh = 28*28,50\n",
    "def get_model(): return nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[CompletionCB()])\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c14d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SingleBatchCB(Callback):\n",
    "    order = 1\n",
    "    def after_batch(self, learn): raise CancelFitException()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29310ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(get_model(), dls, F.cross_entropy, lr=0.2, cbs=[SingleBatchCB(), CompletionCB()])\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559c0986",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ffc6fc-c244-4ad9-8bb3-be9150a790bc",
   "metadata": {},
   "source": [
    "How do we do things other than multiclass accuracy?? We could create a `Metric` class - within which we will define subclasses which calculate particular metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f810c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    def __init__(self): self.reset()\n",
    "    def reset(self): self.vals,self.ns = [],[]\n",
    "    def add(self, inp, targ=None, n=1):\n",
    "        self.last = self.calc(inp, targ)\n",
    "        self.vals.append(self.last)\n",
    "        self.ns.append(n)\n",
    "    @property\n",
    "    def value(self):\n",
    "        ns = tensor(self.ns)\n",
    "        return (tensor(self.vals)*ns).sum()/ns.sum()\n",
    "    def calc(self, inps, targs): return inps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55286ed-e563-479a-8761-84048870a27c",
   "metadata": {},
   "source": [
    "The above is just going to be used for loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "092dd298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(Metric):\n",
    "    def calc(self, inps, targs): return (inps==targs).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53229b-6f4a-4d57-94bc-b09de989f8f6",
   "metadata": {},
   "source": [
    "In the above code, we define a subclass `Accuracy` - one that inherits from `Metric` - but the definition of `calc()` is modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87752b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.45)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = Accuracy()\n",
    "acc.add(tensor([0, 1, 2, 0, 1, 2]), tensor([0, 1, 1, 2, 1, 0])) #add in a minibatch of inputs and predictions\n",
    "acc.add(tensor([1, 1, 2, 0, 1]), tensor([0, 1, 1, 2, 1]))\n",
    "acc.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da1843-0e79-49e2-86d7-fb61fd00a035",
   "metadata": {},
   "source": [
    "`acc.value` does not have a `()`, because it is a property!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dff514-270a-49e1-a4d2-25dfde30810a",
   "metadata": {},
   "source": [
    "And then, just use Metric() for the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdf18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Metric()\n",
    "loss.add(0.6, n=32)\n",
    "loss.add(0.9, n=2)\n",
    "loss.value, round((0.6*32+0.9*2)/(32+2), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca935dc",
   "metadata": {},
   "source": [
    "## Some callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16df8dd",
   "metadata": {},
   "source": [
    "```\n",
    "pip install torcheval\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torcheval.metrics import MulticlassAccuracy,Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee473e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MulticlassAccuracy()\n",
    "metric.update(tensor([0, 2, 1, 3]), tensor([0, 1, 2, 3]))\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c531ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.reset()\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8692bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_cpu(x):\n",
    "    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n",
    "    if isinstance(x, list): return [to_cpu(o) for o in x]\n",
    "    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n",
    "    res = x.detach().cpu()\n",
    "    return res.float() if res.dtype==torch.float16 else res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678712dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MetricsCB(Callback):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        for o in ms: metrics[type(o).__name__] = o\n",
    "        self.metrics = metrics\n",
    "        self.all_metrics = copy(metrics)\n",
    "        self.all_metrics['loss'] = self.loss = Mean()\n",
    "\n",
    "    def _log(self, d): print(d)\n",
    "    def before_fit(self, learn): learn.metrics = self\n",
    "    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n",
    "\n",
    "    def after_epoch(self, learn):\n",
    "        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n",
    "        log['epoch'] = learn.epoch\n",
    "        log['train'] = 'train' if learn.model.training else 'eval'\n",
    "        self._log(log)\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        x,y,*_ = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339962be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): fc.store_attr()\n",
    "    def before_fit(self, learn):\n",
    "        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n",
    "    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce9364",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=[DeviceCB(), metrics])\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24420ba4",
   "metadata": {},
   "source": [
    "## Flexible learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1732aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n",
    "        cbs = fc.L(cbs)\n",
    "        fc.store_attr()\n",
    "\n",
    "    @contextmanager\n",
    "    def cb_ctx(self, nm):\n",
    "        try:\n",
    "            self.callback(f'before_{nm}')\n",
    "            yield\n",
    "            self.callback(f'after_{nm}')\n",
    "        except globals()[f'Cancel{nm.title()}Exception']: pass\n",
    "        finally: self.callback(f'cleanup_{nm}')\n",
    "                \n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        with self.cb_ctx('epoch'):\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                with self.cb_ctx('batch'):\n",
    "                    self.predict()\n",
    "                    self.get_loss()\n",
    "                    if self.training:\n",
    "                        self.backward()\n",
    "                        self.step()\n",
    "                        self.zero_grad()\n",
    "    \n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n",
    "        cbs = fc.L(cbs)\n",
    "        # `add_cb` and `rm_cb` were added in lesson 18\n",
    "        for cb in cbs: self.cbs.append(cb)\n",
    "        try:\n",
    "            self.n_epochs = n_epochs\n",
    "            self.epochs = range(n_epochs)\n",
    "            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)\n",
    "            with self.cb_ctx('fit'):\n",
    "                for self.epoch in self.epochs:\n",
    "                    if train: self.one_epoch(True)\n",
    "                    if valid: torch.no_grad()(self.one_epoch)(False)\n",
    "        finally:\n",
    "            for cb in cbs: self.cbs.remove(cb)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "    \n",
    "    @property\n",
    "    def training(self): return self.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee3643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TrainCB(Callback):\n",
    "    def __init__(self, n_inp=1): self.n_inp = n_inp\n",
    "    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])\n",
    "    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e6eb3",
   "metadata": {},
   "source": [
    "NB: I added `self.n_inp` after the lesson. This allows us to train models with more than one input or output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9537f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressCB(Callback):\n",
    "    order = MetricsCB.order+1\n",
    "    def __init__(self, plot=False): self.plot = plot\n",
    "    def before_fit(self, learn):\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n",
    "        self.losses = []\n",
    "\n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "\n",
    "    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "    def after_batch(self, learn):\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.training:\n",
    "            self.losses.append(learn.loss.item())\n",
    "            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82dcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b77daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "cbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978f0fe",
   "metadata": {},
   "source": [
    "## Updated versions since the lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c38064",
   "metadata": {},
   "source": [
    "After the lesson we noticed that `contextlib.context_manager` has a surprising \"feature\" which doesn't let us raise an exception before the `yield`. Therefore we've replaced the context manager with a decorator in this updated version of `Learner`. We have also added a few more callbacks in `one_epoch()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ddb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class with_cbs:\n",
    "    def __init__(self, nm): self.nm = nm\n",
    "    def __call__(self, f):\n",
    "        def _f(o, *args, **kwargs):\n",
    "            try:\n",
    "                o.callback(f'before_{self.nm}')\n",
    "                f(o, *args, **kwargs)\n",
    "                o.callback(f'after_{self.nm}')\n",
    "            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n",
    "            finally: o.callback(f'cleanup_{self.nm}')\n",
    "        return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Learner():\n",
    "    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n",
    "        cbs = fc.L(cbs)\n",
    "        fc.store_attr()\n",
    "\n",
    "    @with_cbs('batch')\n",
    "    def _one_batch(self):\n",
    "        self.predict()\n",
    "        self.callback('after_predict')\n",
    "        self.get_loss()\n",
    "        self.callback('after_loss')\n",
    "        if self.training:\n",
    "            self.backward()\n",
    "            self.callback('after_backward')\n",
    "            self.step()\n",
    "            self.callback('after_step')\n",
    "            self.zero_grad()\n",
    "\n",
    "    @with_cbs('epoch')\n",
    "    def _one_epoch(self):\n",
    "        for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n",
    "\n",
    "    def one_epoch(self, training):\n",
    "        self.model.train(training)\n",
    "        self.dl = self.dls.train if training else self.dls.valid\n",
    "        self._one_epoch()\n",
    "\n",
    "    @with_cbs('fit')\n",
    "    def _fit(self, train, valid):\n",
    "        for self.epoch in self.epochs:\n",
    "            if train: self.one_epoch(True)\n",
    "            if valid: torch.no_grad()(self.one_epoch)(False)\n",
    "\n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n",
    "        cbs = fc.L(cbs)\n",
    "        # `add_cb` and `rm_cb` were added in lesson 18\n",
    "        for cb in cbs: self.cbs.append(cb)\n",
    "        try:\n",
    "            self.n_epochs = n_epochs\n",
    "            self.epochs = range(n_epochs)\n",
    "            if lr is None: lr = self.lr\n",
    "            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n",
    "            self._fit(train, valid)\n",
    "        finally:\n",
    "            for cb in cbs: self.cbs.remove(cb)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "    \n",
    "    @property\n",
    "    def training(self): return self.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08159e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "cbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36aef26",
   "metadata": {},
   "source": [
    "## TrainLearner and MomentumLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TrainLearner(Learner):\n",
    "    def predict(self): self.preds = self.model(self.batch[0])\n",
    "    def get_loss(self): self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "    def backward(self): self.loss.backward()\n",
    "    def step(self): self.opt.step()\n",
    "    def zero_grad(self): self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68148d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MomentumLearner(TrainLearner):\n",
    "    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=optim.SGD, mom=0.85):\n",
    "        self.mom = mom\n",
    "        super().__init__(model, dls, loss_func, lr, cbs, opt_func)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.model.parameters(): p.grad *= self.mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452eff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: No TrainCB\n",
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True)]\n",
    "learn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=0.1, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e3f750",
   "metadata": {},
   "source": [
    "## LRFinderCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b9f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinderCB(Callback):\n",
    "    def __init__(self, lr_mult=1.3): fc.store_attr()\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        self.lrs,self.losses = [],[]\n",
    "        self.min = math.inf\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        if not learn.training: raise CancelEpochException()\n",
    "        self.lrs.append(learn.opt.param_groups[0]['lr'])\n",
    "        loss = to_cpu(learn.loss)\n",
    "        self.losses.append(loss)\n",
    "        if loss < self.min: self.min = loss\n",
    "        if loss > self.min*3: raise CancelFitException()\n",
    "        for g in learn.opt.param_groups: g['lr'] *= self.lr_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09da2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfind = LRFinderCB()\n",
    "cbs = [DeviceCB(), lrfind]\n",
    "learn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-4, cbs=cbs)\n",
    "learn.fit(1)\n",
    "plt.plot(lrfind.lrs, lrfind.losses)\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f61663",
   "metadata": {},
   "source": [
    "[ExponentialLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LRFinderCB(Callback):\n",
    "    def __init__(self, gamma=1.3, max_mult=3): fc.store_attr()\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        self.sched = ExponentialLR(learn.opt, self.gamma)\n",
    "        self.lrs,self.losses = [],[]\n",
    "        self.min = math.inf\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        if not learn.training: raise CancelEpochException()\n",
    "        self.lrs.append(learn.opt.param_groups[0]['lr'])\n",
    "        loss = to_cpu(learn.loss)\n",
    "        self.losses.append(loss)\n",
    "        if loss < self.min: self.min = loss\n",
    "        if math.isnan(loss) or (loss > self.min*self.max_mult):\n",
    "            raise CancelFitException()\n",
    "        self.sched.step()\n",
    "\n",
    "    def cleanup_fit(self, learn):\n",
    "        plt.plot(self.lrs, self.losses)\n",
    "        plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50956a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [DeviceCB()]\n",
    "learn = MomentumLearner(get_model(), dls, F.cross_entropy, lr=1e-5, cbs=cbs)\n",
    "learn.fit(3, cbs=LRFinderCB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff226c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def lr_find(self:Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):\n",
    "    self.fit(max_epochs, lr=start_lr, cbs=LRFinderCB(gamma=gamma, max_mult=max_mult))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281c3eb",
   "metadata": {},
   "source": [
    "`lr_find` was added in lesson 18. It's just a shorter way of using `LRFinderCB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MomentumLearner(get_model(), dls, F.cross_entropy, cbs=cbs).lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb9bd2",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465118f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc774ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
